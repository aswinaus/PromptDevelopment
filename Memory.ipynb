{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file\n",
    "\n",
    "# API configuration\n",
    "os.environ[\"OpenAI_API_Key\"]=\"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\"\n",
    "openai.api_key=os.getenv(\"OpenAI_API_Key\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# account for deprecation of LLM model\n",
    "import datetime\n",
    "# Get the current date\n",
    "current_date = datetime.datetime.now().date()\n",
    "\n",
    "# Define the date after which the model should be set to \"gpt-3.5-turbo\"\n",
    "target_date = datetime.date(2024, 6, 12)\n",
    "\n",
    "# Set the model variable based on the current date\n",
    "if current_date > target_date:\n",
    "    llm_model = \"gpt-3.5-turbo\"\n",
    "else:\n",
    "    llm_model = \"gpt-3.5-turbo-0301\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0.0, model=llm_model)\n",
    "memory = ConversationBufferMemory()\n",
    "conversation = ConversationChain(\n",
    "    llm=llm, \n",
    "    memory = memory,\n",
    "    verbose=True #setting the verbose to true will help understand how memory works in Langchain\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi, my name is Aswin\n",
      "AI: Hello Aswin, it's nice to meet you! I am an AI language model designed to assist with various tasks and answer questions. How can I assist you today?\n",
      "Human: What is 1+1?\n",
      "AI: The answer to 1+1 is 2.\n",
      "Human: Hi, my name is Aswin\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Hello again, Aswin! It's nice to see you. Is there anything else I can help you with?\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"Hi, my name is Aswin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi, my name is Aswin\n",
      "AI: Hello Aswin, it's nice to meet you! I am an AI language model designed to assist with various tasks and answer questions. How can I assist you today?\n",
      "Human: What is 1+1?\n",
      "AI: The answer to 1+1 is 2.\n",
      "Human: Hi, my name is Aswin\n",
      "AI: Hello again, Aswin! It's nice to see you. Is there anything else I can help you with?\n",
      "Human: What is 1+1?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'As I mentioned earlier, the answer to 1+1 is 2. Is there anything else you would like to know?'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"What is 1+1?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi, my name is Aswin\n",
      "AI: Hello Aswin, it's nice to meet you! I am an AI language model designed to assist with various tasks and answer questions. How can I assist you today?\n",
      "Human: What is 1+1?\n",
      "AI: The answer to 1+1 is 2.\n",
      "Human: Hi, my name is Aswin\n",
      "AI: Hello again, Aswin! It's nice to see you. Is there anything else I can help you with?\n",
      "Human: What is 1+1?\n",
      "AI: As I mentioned earlier, the answer to 1+1 is 2. Is there anything else you would like to know?\n",
      "Human: What is my name?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Your name is Aswin, as you mentioned earlier.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"What is my name?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Hi, my name is Aswin\n",
      "AI: Hello Aswin, it's nice to meet you! I am an AI language model designed to assist with various tasks and answer questions. How can I assist you today?\n",
      "Human: What is 1+1?\n",
      "AI: The answer to 1+1 is 2.\n",
      "Human: Hi, my name is Aswin\n",
      "AI: Hello again, Aswin! It's nice to see you. Is there anything else I can help you with?\n",
      "Human: What is 1+1?\n",
      "AI: As I mentioned earlier, the answer to 1+1 is 2. Is there anything else you would like to know?\n",
      "Human: What is my name?\n",
      "AI: Your name is Aswin, as you mentioned earlier.\n"
     ]
    }
   ],
   "source": [
    "print(memory.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': \"Human: Hi, my name is Aswin\\nAI: Hello Aswin, it's nice to meet you! I am an AI language model designed to assist with various tasks and answer questions. How can I assist you today?\\nHuman: What is 1+1?\\nAI: The answer to 1+1 is 2.\\nHuman: Hi, my name is Aswin\\nAI: Hello again, Aswin! It's nice to see you. Is there anything else I can help you with?\\nHuman: What is 1+1?\\nAI: As I mentioned earlier, the answer to 1+1 is 2. Is there anything else you would like to know?\\nHuman: What is my name?\\nAI: Your name is Aswin, as you mentioned earlier.\"}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationBufferMemory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.save_context({\"input\": \"Hi\"}, \n",
    "                    {\"output\": \"What's up\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: Hi\n",
      "AI: What's up\n"
     ]
    }
   ],
   "source": [
    "print(memory.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': \"Human: Hi\\nAI: What's up\"}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.save_context({\"input\": \"Not much, just hanging\"}, \n",
    "                    {\"output\": \"Cool\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': \"Human: Hi\\nAI: What's up\\nHuman: Not much, just hanging\\nAI: Cool\"}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ConversationBufferWindowMemory\n",
    "from langchain.memory import ConversationBufferWindowMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationBufferWindowMemory(k=1)               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.save_context({\"input\": \"Hi\"},\n",
    "                    {\"output\": \"What's up\"})\n",
    "memory.save_context({\"input\": \"Not much, just hanging\"},\n",
    "                    {\"output\": \"Cool\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'Human: Not much, just hanging\\nAI: Cool'}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0.0, model=llm_model)\n",
    "memory = ConversationBufferWindowMemory(k=1)\n",
    "conversation = ConversationChain(\n",
    "    llm=llm, \n",
    "    memory = memory,\n",
    "    verbose=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hello Aswin, it's nice to meet you! I am an AI language model designed to assist with various tasks and answer questions. How can I assist you today?\""
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"Hi, my name is Aswin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The answer to 1+1 is 2.'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"What is 1+1?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'm sorry, I still don't have access to that information. However, based on our conversation so far, I can tell you that you seem like a curious and friendly person. Is there anything else I can help you with?\""
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"What is my name?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tiktoken in c:\\users\\aswin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.3.1)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\aswin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tiktoken) (2023.8.8)\n",
      "Requirement already satisfied: requests>=2.26.0 in c:\\users\\aswin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tiktoken) (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\aswin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests>=2.26.0->tiktoken) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\aswin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests>=2.26.0->tiktoken) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\aswin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests>=2.26.0->tiktoken) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\aswin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2023.7.22)\n",
      "Requirement already satisfied: tiktoken==0.3.1 in c:\\users\\aswin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.3.1)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\aswin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tiktoken==0.3.1) (2023.8.8)\n",
      "Requirement already satisfied: requests>=2.26.0 in c:\\users\\aswin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tiktoken==0.3.1) (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\aswin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests>=2.26.0->tiktoken==0.3.1) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\aswin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests>=2.26.0->tiktoken==0.3.1) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\aswin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests>=2.26.0->tiktoken==0.3.1) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\aswin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests>=2.26.0->tiktoken==0.3.1) (2023.7.22)\n"
     ]
    }
   ],
   "source": [
    "### ConversationTokenBufferMemory\n",
    "! pip install tiktoken\n",
    "! pip install tiktoken==0.3.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationTokenBufferMemory\n",
    "from langchain.llms import OpenAI\n",
    "llm = ChatOpenAI(temperature=0.0, model=llm_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Could not import tiktoken python package. This is needed in order to calculate get_token_ids. Please install it with `pip install tiktoken`.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\aswin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain\\chat_models\\openai.py:49\u001b[0m, in \u001b[0;36m_import_tiktoken\u001b[1;34m()\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 49\u001b[0m     \u001b[39mimport\u001b[39;00m \u001b[39mtiktoken\u001b[39;00m\n\u001b[0;32m     50\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mImportError\u001b[39;00m:\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tiktoken'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\aswin\\OneDrive\\Desktop\\Code\\OpenAI\\Prompt\\Memory.ipynb Cell 27\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/aswin/OneDrive/Desktop/Code/OpenAI/Prompt/Memory.ipynb#X35sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m memory \u001b[39m=\u001b[39m ConversationTokenBufferMemory(llm\u001b[39m=\u001b[39mllm, max_token_limit\u001b[39m=\u001b[39m\u001b[39m50\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/aswin/OneDrive/Desktop/Code/OpenAI/Prompt/Memory.ipynb#X35sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m memory\u001b[39m.\u001b[39;49msave_context({\u001b[39m\"\u001b[39;49m\u001b[39minput\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m\"\u001b[39;49m\u001b[39mAI is what?!\u001b[39;49m\u001b[39m\"\u001b[39;49m},\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/aswin/OneDrive/Desktop/Code/OpenAI/Prompt/Memory.ipynb#X35sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m                     {\u001b[39m\"\u001b[39;49m\u001b[39moutput\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m\"\u001b[39;49m\u001b[39mAmazing!\u001b[39;49m\u001b[39m\"\u001b[39;49m})\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/aswin/OneDrive/Desktop/Code/OpenAI/Prompt/Memory.ipynb#X35sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m memory\u001b[39m.\u001b[39msave_context({\u001b[39m\"\u001b[39m\u001b[39minput\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mBackpropagation is what?\u001b[39m\u001b[39m\"\u001b[39m},\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/aswin/OneDrive/Desktop/Code/OpenAI/Prompt/Memory.ipynb#X35sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m                     {\u001b[39m\"\u001b[39m\u001b[39moutput\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mBeautiful!\u001b[39m\u001b[39m\"\u001b[39m})\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/aswin/OneDrive/Desktop/Code/OpenAI/Prompt/Memory.ipynb#X35sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m memory\u001b[39m.\u001b[39msave_context({\u001b[39m\"\u001b[39m\u001b[39minput\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mChatbots are what?\u001b[39m\u001b[39m\"\u001b[39m}, \n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/aswin/OneDrive/Desktop/Code/OpenAI/Prompt/Memory.ipynb#X35sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m                     {\u001b[39m\"\u001b[39m\u001b[39moutput\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mCharming!\u001b[39m\u001b[39m\"\u001b[39m})\n",
      "File \u001b[1;32mc:\\Users\\aswin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain\\memory\\token_buffer.py:53\u001b[0m, in \u001b[0;36mConversationTokenBufferMemory.save_context\u001b[1;34m(self, inputs, outputs)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[39m# Prune buffer if it exceeds max token limit\u001b[39;00m\n\u001b[0;32m     52\u001b[0m buffer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchat_memory\u001b[39m.\u001b[39mmessages\n\u001b[1;32m---> 53\u001b[0m curr_buffer_length \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mllm\u001b[39m.\u001b[39;49mget_num_tokens_from_messages(buffer)\n\u001b[0;32m     54\u001b[0m \u001b[39mif\u001b[39;00m curr_buffer_length \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_token_limit:\n\u001b[0;32m     55\u001b[0m     pruned_memory \u001b[39m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Users\\aswin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain\\chat_models\\openai.py:492\u001b[0m, in \u001b[0;36mChatOpenAI.get_num_tokens_from_messages\u001b[1;34m(self, messages)\u001b[0m\n\u001b[0;32m    490\u001b[0m \u001b[39mif\u001b[39;00m sys\u001b[39m.\u001b[39mversion_info[\u001b[39m1\u001b[39m] \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m7\u001b[39m:\n\u001b[0;32m    491\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mget_num_tokens_from_messages(messages)\n\u001b[1;32m--> 492\u001b[0m model, encoding \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_encoding_model()\n\u001b[0;32m    493\u001b[0m \u001b[39mif\u001b[39;00m model\u001b[39m.\u001b[39mstartswith(\u001b[39m\"\u001b[39m\u001b[39mgpt-3.5-turbo-0301\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m    494\u001b[0m     \u001b[39m# every message follows <im_start>{role/name}\\n{content}<im_end>\\n\u001b[39;00m\n\u001b[0;32m    495\u001b[0m     tokens_per_message \u001b[39m=\u001b[39m \u001b[39m4\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\aswin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain\\chat_models\\openai.py:455\u001b[0m, in \u001b[0;36mChatOpenAI._get_encoding_model\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    454\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_get_encoding_model\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[\u001b[39mstr\u001b[39m, tiktoken\u001b[39m.\u001b[39mEncoding]:\n\u001b[1;32m--> 455\u001b[0m     tiktoken_ \u001b[39m=\u001b[39m _import_tiktoken()\n\u001b[0;32m    456\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtiktoken_model_name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    457\u001b[0m         model \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtiktoken_model_name\n",
      "File \u001b[1;32mc:\\Users\\aswin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain\\chat_models\\openai.py:51\u001b[0m, in \u001b[0;36m_import_tiktoken\u001b[1;34m()\u001b[0m\n\u001b[0;32m     49\u001b[0m     \u001b[39mimport\u001b[39;00m \u001b[39mtiktoken\u001b[39;00m\n\u001b[0;32m     50\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mImportError\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m     52\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mCould not import tiktoken python package. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     53\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mThis is needed in order to calculate get_token_ids. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     54\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mPlease install it with `pip install tiktoken`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     55\u001b[0m     )\n\u001b[0;32m     56\u001b[0m \u001b[39mreturn\u001b[39;00m tiktoken\n",
      "\u001b[1;31mValueError\u001b[0m: Could not import tiktoken python package. This is needed in order to calculate get_token_ids. Please install it with `pip install tiktoken`."
     ]
    }
   ],
   "source": [
    "\n",
    "memory = ConversationTokenBufferMemory(llm=llm, max_token_limit=50)\n",
    "memory.save_context({\"input\": \"AI is what?!\"},\n",
    "                    {\"output\": \"Amazing!\"})\n",
    "memory.save_context({\"input\": \"Backpropagation is what?\"},\n",
    "                    {\"output\": \"Beautiful!\"})\n",
    "memory.save_context({\"input\": \"Chatbots are what?\"}, \n",
    "                    {\"output\": \"Charming!\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'Human: AI is what?!\\nAI: Amazing!'}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ConversationSummaryMemory\n",
    "from langchain.memory import ConversationSummaryBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Could not import tiktoken python package. This is needed in order to calculate get_token_ids. Please install it with `pip install tiktoken`.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\aswin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain\\chat_models\\openai.py:49\u001b[0m, in \u001b[0;36m_import_tiktoken\u001b[1;34m()\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 49\u001b[0m     \u001b[39mimport\u001b[39;00m \u001b[39mtiktoken\u001b[39;00m\n\u001b[0;32m     50\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mImportError\u001b[39;00m:\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tiktoken'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\aswin\\OneDrive\\Desktop\\Code\\OpenAI\\Prompt\\Memory.ipynb Cell 30\u001b[0m line \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/aswin/OneDrive/Desktop/Code/OpenAI/Prompt/Memory.ipynb#X41sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m schedule \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mThere is a meeting at 8am with your product team. \u001b[39m\u001b[39m\\\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/aswin/OneDrive/Desktop/Code/OpenAI/Prompt/Memory.ipynb#X41sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mYou will need your powerpoint presentation prepared. \u001b[39m\u001b[39m\\\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/aswin/OneDrive/Desktop/Code/OpenAI/Prompt/Memory.ipynb#X41sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m9am-12pm have time to work on your LangChain \u001b[39m\u001b[39m\\\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/aswin/OneDrive/Desktop/Code/OpenAI/Prompt/Memory.ipynb#X41sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mfrom over an hour away to meet you to understand the latest in AI. \u001b[39m\u001b[39m\\\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/aswin/OneDrive/Desktop/Code/OpenAI/Prompt/Memory.ipynb#X41sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mBe sure to bring your laptop to show the latest LLM demo.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/aswin/OneDrive/Desktop/Code/OpenAI/Prompt/Memory.ipynb#X41sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m memory \u001b[39m=\u001b[39m ConversationSummaryBufferMemory(llm\u001b[39m=\u001b[39mllm, max_token_limit\u001b[39m=\u001b[39m\u001b[39m100\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/aswin/OneDrive/Desktop/Code/OpenAI/Prompt/Memory.ipynb#X41sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m memory\u001b[39m.\u001b[39;49msave_context({\u001b[39m\"\u001b[39;49m\u001b[39minput\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m\"\u001b[39;49m\u001b[39mHello\u001b[39;49m\u001b[39m\"\u001b[39;49m}, {\u001b[39m\"\u001b[39;49m\u001b[39moutput\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m\"\u001b[39;49m\u001b[39mWhat\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39ms up\u001b[39;49m\u001b[39m\"\u001b[39;49m})\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/aswin/OneDrive/Desktop/Code/OpenAI/Prompt/Memory.ipynb#X41sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m memory\u001b[39m.\u001b[39msave_context({\u001b[39m\"\u001b[39m\u001b[39minput\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mNot much, just hanging\u001b[39m\u001b[39m\"\u001b[39m},\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/aswin/OneDrive/Desktop/Code/OpenAI/Prompt/Memory.ipynb#X41sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m                     {\u001b[39m\"\u001b[39m\u001b[39moutput\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mCool\u001b[39m\u001b[39m\"\u001b[39m})\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/aswin/OneDrive/Desktop/Code/OpenAI/Prompt/Memory.ipynb#X41sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m memory\u001b[39m.\u001b[39msave_context({\u001b[39m\"\u001b[39m\u001b[39minput\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mWhat is on the schedule today?\u001b[39m\u001b[39m\"\u001b[39m}, \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/aswin/OneDrive/Desktop/Code/OpenAI/Prompt/Memory.ipynb#X41sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m                     {\u001b[39m\"\u001b[39m\u001b[39moutput\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mschedule\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m})\n",
      "File \u001b[1;32mc:\\Users\\aswin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain\\memory\\summary_buffer.py:59\u001b[0m, in \u001b[0;36mConversationSummaryBufferMemory.save_context\u001b[1;34m(self, inputs, outputs)\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Save context from this conversation to buffer.\"\"\"\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39msave_context(inputs, outputs)\n\u001b[1;32m---> 59\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprune()\n",
      "File \u001b[1;32mc:\\Users\\aswin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain\\memory\\summary_buffer.py:64\u001b[0m, in \u001b[0;36mConversationSummaryBufferMemory.prune\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Prune buffer if it exceeds max token limit\"\"\"\u001b[39;00m\n\u001b[0;32m     63\u001b[0m buffer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchat_memory\u001b[39m.\u001b[39mmessages\n\u001b[1;32m---> 64\u001b[0m curr_buffer_length \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mllm\u001b[39m.\u001b[39;49mget_num_tokens_from_messages(buffer)\n\u001b[0;32m     65\u001b[0m \u001b[39mif\u001b[39;00m curr_buffer_length \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_token_limit:\n\u001b[0;32m     66\u001b[0m     pruned_memory \u001b[39m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Users\\aswin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain\\chat_models\\openai.py:492\u001b[0m, in \u001b[0;36mChatOpenAI.get_num_tokens_from_messages\u001b[1;34m(self, messages)\u001b[0m\n\u001b[0;32m    490\u001b[0m \u001b[39mif\u001b[39;00m sys\u001b[39m.\u001b[39mversion_info[\u001b[39m1\u001b[39m] \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m7\u001b[39m:\n\u001b[0;32m    491\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mget_num_tokens_from_messages(messages)\n\u001b[1;32m--> 492\u001b[0m model, encoding \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_encoding_model()\n\u001b[0;32m    493\u001b[0m \u001b[39mif\u001b[39;00m model\u001b[39m.\u001b[39mstartswith(\u001b[39m\"\u001b[39m\u001b[39mgpt-3.5-turbo-0301\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m    494\u001b[0m     \u001b[39m# every message follows <im_start>{role/name}\\n{content}<im_end>\\n\u001b[39;00m\n\u001b[0;32m    495\u001b[0m     tokens_per_message \u001b[39m=\u001b[39m \u001b[39m4\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\aswin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain\\chat_models\\openai.py:455\u001b[0m, in \u001b[0;36mChatOpenAI._get_encoding_model\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    454\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_get_encoding_model\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[\u001b[39mstr\u001b[39m, tiktoken\u001b[39m.\u001b[39mEncoding]:\n\u001b[1;32m--> 455\u001b[0m     tiktoken_ \u001b[39m=\u001b[39m _import_tiktoken()\n\u001b[0;32m    456\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtiktoken_model_name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    457\u001b[0m         model \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtiktoken_model_name\n",
      "File \u001b[1;32mc:\\Users\\aswin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain\\chat_models\\openai.py:51\u001b[0m, in \u001b[0;36m_import_tiktoken\u001b[1;34m()\u001b[0m\n\u001b[0;32m     49\u001b[0m     \u001b[39mimport\u001b[39;00m \u001b[39mtiktoken\u001b[39;00m\n\u001b[0;32m     50\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mImportError\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m     52\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mCould not import tiktoken python package. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     53\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mThis is needed in order to calculate get_token_ids. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     54\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mPlease install it with `pip install tiktoken`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     55\u001b[0m     )\n\u001b[0;32m     56\u001b[0m \u001b[39mreturn\u001b[39;00m tiktoken\n",
      "\u001b[1;31mValueError\u001b[0m: Could not import tiktoken python package. This is needed in order to calculate get_token_ids. Please install it with `pip install tiktoken`."
     ]
    }
   ],
   "source": [
    "# create a long string\n",
    "schedule = \"There is a meeting at 8am with your product team. \\\n",
    "You will need your powerpoint presentation prepared. \\\n",
    "9am-12pm have time to work on your LangChain \\\n",
    "project which will go quickly because Langchain is such a powerful tool. \\\n",
    "At Noon, lunch at the italian resturant with a customer who is driving \\\n",
    "from over an hour away to meet you to understand the latest in AI. \\\n",
    "Be sure to bring your laptop to show the latest LLM demo.\"\n",
    "\n",
    "memory = ConversationSummaryBufferMemory(llm=llm, max_token_limit=100)\n",
    "memory.save_context({\"input\": \"Hello\"}, {\"output\": \"What's up\"})\n",
    "memory.save_context({\"input\": \"Not much, just hanging\"},\n",
    "                    {\"output\": \"Cool\"})\n",
    "memory.save_context({\"input\": \"What is on the schedule today?\"}, \n",
    "                    {\"output\": f\"{schedule}\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': \"Human: Hello\\nAI: What's up\"}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation = ConversationChain(\n",
    "    llm=llm, \n",
    "    memory = memory,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hello\n",
      "AI: What's up\n",
      "Human: What would be a good demo to show?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Could not import tiktoken python package. This is needed in order to calculate get_token_ids. Please install it with `pip install tiktoken`.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\aswin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain\\chat_models\\openai.py:49\u001b[0m, in \u001b[0;36m_import_tiktoken\u001b[1;34m()\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 49\u001b[0m     \u001b[39mimport\u001b[39;00m \u001b[39mtiktoken\u001b[39;00m\n\u001b[0;32m     50\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mImportError\u001b[39;00m:\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tiktoken'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\aswin\\OneDrive\\Desktop\\Code\\OpenAI\\Prompt\\Memory.ipynb Cell 33\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/aswin/OneDrive/Desktop/Code/OpenAI/Prompt/Memory.ipynb#X44sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m conversation\u001b[39m.\u001b[39;49mpredict(\u001b[39minput\u001b[39;49m\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mWhat would be a good demo to show?\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\aswin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain\\chains\\llm.py:255\u001b[0m, in \u001b[0;36mLLMChain.predict\u001b[1;34m(self, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    240\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpredict\u001b[39m(\u001b[39mself\u001b[39m, callbacks: Callbacks \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mstr\u001b[39m:\n\u001b[0;32m    241\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Format prompt with kwargs and pass to LLM.\u001b[39;00m\n\u001b[0;32m    242\u001b[0m \n\u001b[0;32m    243\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    253\u001b[0m \u001b[39m            completion = llm.predict(adjective=\"funny\")\u001b[39;00m\n\u001b[0;32m    254\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 255\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m(kwargs, callbacks\u001b[39m=\u001b[39;49mcallbacks)[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_key]\n",
      "File \u001b[1;32mc:\\Users\\aswin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain\\chains\\base.py:284\u001b[0m, in \u001b[0;36mChain.__call__\u001b[1;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, include_run_info)\u001b[0m\n\u001b[0;32m    282\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[0;32m    283\u001b[0m run_manager\u001b[39m.\u001b[39mon_chain_end(outputs)\n\u001b[1;32m--> 284\u001b[0m final_outputs: Dict[\u001b[39mstr\u001b[39m, Any] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprep_outputs(\n\u001b[0;32m    285\u001b[0m     inputs, outputs, return_only_outputs\n\u001b[0;32m    286\u001b[0m )\n\u001b[0;32m    287\u001b[0m \u001b[39mif\u001b[39;00m include_run_info:\n\u001b[0;32m    288\u001b[0m     final_outputs[RUN_KEY] \u001b[39m=\u001b[39m RunInfo(run_id\u001b[39m=\u001b[39mrun_manager\u001b[39m.\u001b[39mrun_id)\n",
      "File \u001b[1;32mc:\\Users\\aswin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain\\chains\\base.py:378\u001b[0m, in \u001b[0;36mChain.prep_outputs\u001b[1;34m(self, inputs, outputs, return_only_outputs)\u001b[0m\n\u001b[0;32m    376\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_outputs(outputs)\n\u001b[0;32m    377\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmemory \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 378\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmemory\u001b[39m.\u001b[39;49msave_context(inputs, outputs)\n\u001b[0;32m    379\u001b[0m \u001b[39mif\u001b[39;00m return_only_outputs:\n\u001b[0;32m    380\u001b[0m     \u001b[39mreturn\u001b[39;00m outputs\n",
      "File \u001b[1;32mc:\\Users\\aswin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain\\memory\\summary_buffer.py:59\u001b[0m, in \u001b[0;36mConversationSummaryBufferMemory.save_context\u001b[1;34m(self, inputs, outputs)\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Save context from this conversation to buffer.\"\"\"\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39msave_context(inputs, outputs)\n\u001b[1;32m---> 59\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprune()\n",
      "File \u001b[1;32mc:\\Users\\aswin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain\\memory\\summary_buffer.py:64\u001b[0m, in \u001b[0;36mConversationSummaryBufferMemory.prune\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Prune buffer if it exceeds max token limit\"\"\"\u001b[39;00m\n\u001b[0;32m     63\u001b[0m buffer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchat_memory\u001b[39m.\u001b[39mmessages\n\u001b[1;32m---> 64\u001b[0m curr_buffer_length \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mllm\u001b[39m.\u001b[39;49mget_num_tokens_from_messages(buffer)\n\u001b[0;32m     65\u001b[0m \u001b[39mif\u001b[39;00m curr_buffer_length \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_token_limit:\n\u001b[0;32m     66\u001b[0m     pruned_memory \u001b[39m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Users\\aswin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain\\chat_models\\openai.py:492\u001b[0m, in \u001b[0;36mChatOpenAI.get_num_tokens_from_messages\u001b[1;34m(self, messages)\u001b[0m\n\u001b[0;32m    490\u001b[0m \u001b[39mif\u001b[39;00m sys\u001b[39m.\u001b[39mversion_info[\u001b[39m1\u001b[39m] \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m7\u001b[39m:\n\u001b[0;32m    491\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mget_num_tokens_from_messages(messages)\n\u001b[1;32m--> 492\u001b[0m model, encoding \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_encoding_model()\n\u001b[0;32m    493\u001b[0m \u001b[39mif\u001b[39;00m model\u001b[39m.\u001b[39mstartswith(\u001b[39m\"\u001b[39m\u001b[39mgpt-3.5-turbo-0301\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m    494\u001b[0m     \u001b[39m# every message follows <im_start>{role/name}\\n{content}<im_end>\\n\u001b[39;00m\n\u001b[0;32m    495\u001b[0m     tokens_per_message \u001b[39m=\u001b[39m \u001b[39m4\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\aswin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain\\chat_models\\openai.py:455\u001b[0m, in \u001b[0;36mChatOpenAI._get_encoding_model\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    454\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_get_encoding_model\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[\u001b[39mstr\u001b[39m, tiktoken\u001b[39m.\u001b[39mEncoding]:\n\u001b[1;32m--> 455\u001b[0m     tiktoken_ \u001b[39m=\u001b[39m _import_tiktoken()\n\u001b[0;32m    456\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtiktoken_model_name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    457\u001b[0m         model \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtiktoken_model_name\n",
      "File \u001b[1;32mc:\\Users\\aswin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain\\chat_models\\openai.py:51\u001b[0m, in \u001b[0;36m_import_tiktoken\u001b[1;34m()\u001b[0m\n\u001b[0;32m     49\u001b[0m     \u001b[39mimport\u001b[39;00m \u001b[39mtiktoken\u001b[39;00m\n\u001b[0;32m     50\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mImportError\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m     52\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mCould not import tiktoken python package. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     53\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mThis is needed in order to calculate get_token_ids. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     54\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mPlease install it with `pip install tiktoken`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     55\u001b[0m     )\n\u001b[0;32m     56\u001b[0m \u001b[39mreturn\u001b[39;00m tiktoken\n",
      "\u001b[1;31mValueError\u001b[0m: Could not import tiktoken python package. This is needed in order to calculate get_token_ids. Please install it with `pip install tiktoken`."
     ]
    }
   ],
   "source": [
    "conversation.predict(input=\"What would be a good demo to show?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': \"Human: Hello\\nAI: What's up\\nHuman: What would be a good demo to show?\\nAI: That depends on the audience and the purpose of the demo. If you're trying to showcase a new product, you could create a demo that highlights its key features and benefits. If you're presenting to investors, you might want to focus on the financial projections and market potential. Ultimately, the best demo is one that effectively communicates your message and resonates with your audience.\"}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
